<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Letian (Max) Fu</title>
  
  <meta name="author" content="Letian (Max) Fu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Letian (Max) Fu</name>
              </p>
              <p>
                I am a first year PhD student at UC Berkeley advised by Professor <a href="https://goldberg.berkeley.edu/">Ken Goldberg</a>. 
                I am interested in building self-supervised multi-modal models and using them in a robotics setting. 
              </p>
              <p>
                I received B.A./M.S. in Computer Science and Applied Mathematics from UC Berkeley in 2023. 
              </p>
              <p style="text-align:center">
                <a href="mailto:max.fu.letian@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=aWot7UgAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/letian-max-fu-8b672b134/">LinkedIn</a> &nbsp/&nbsp
                <a href="CV.pdf">Resume</a>
                <!-- &nbsp/&nbsp -->
              </p>
            </td>
            <td style="padding:2.5%;width:100%;max-width:100%">
              <a href="images/profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 

          <!-- RPT -->
          <tr onmouseout="rpt_stop()" onmouseover="rpt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <div class="two" id='rpt_splash_2'>
                  <video  width=115% height=100% muted autoplay loop>
                    <source src="images/rpt_splash_2.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <video  width=115% height=100% muted autoplay loop id="rpt_splash_1">
                  <source src="images/rpt_splash_1.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function rpt_start() {
                  document.getElementById('rpt_splash_1').style.opacity = "0";
                  document.getElementById('rpt_splash_2').style.opacity = "1";
                }

                function rpt_stop() {
                  document.getElementById('rpt_splash_1').style.opacity = "1";
                  document.getElementById('rpt_splash_2').style.opacity = "0";
                }
                rpt_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <papertitle><a href="https://robotic-pretrained-transformer.github.io/">Robot Learning with Sensorimotor Pre-training</a></papertitle>
              <br>
              <!-- AUTHORS -->
              Ilija Radosavovic, Baifeng Shi, <b>Letian Fu</b>, Ken Goldberg, Trevor Darrell, Jitendra Malik
              <br>
              <!-- CONFERENCE -->
              <em>CoRL</em> 2023, <b>Oral Presentation</b>, <a href="https://openreview.net/forum?id=3gh9hf3R6x">OpenReview</a>
              <!-- <em>Arxiv</em>, <a href="https://arxiv.org/abs/2306.10007">arXiv</a> -->
              <p></p>
              <!-- SUMMARY -->
              <!-- We introduce a self-supervised sensorimotor pre-training method for robotics using a Transformer model called RPT. It encodes sequences of images, robot states, and actions into tokens and trains to predict masked content, enabling the robot to model the physical world. RPT operates on latent visual data, scales to 10x larger models, and allows 10 Hz inference. Through evaluation on 20,000 real-world trajectories collected over 9 months, RPT consistently outperforms training from scratch, doubles improvements in block stacking, and shows beneficial scaling characteristics. -->
            </td>
          </tr>

          <!-- TacInsertion -->
          <tr onmouseout="tacinsert_stop()" onmouseover="tacinsert_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tactile_insertion_2'>
                  <img src="images/tactile_insertion_2.jpg" width='180'/>
                </div>
                <img src="images/tactile_insertion.jpg" width='180' id='tactile_insertion_1'/>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function tacinsert_start() {
                  document.getElementById('tactile_insertion_1').style.opacity = "0";
                  document.getElementById('tactile_insertion_2').style.opacity = "1";
                }

                function tacinsert_stop() {
                  document.getElementById('tactile_insertion_1').style.opacity = "1";
                  document.getElementById('tactile_insertion_2').style.opacity = "0";
                }
                tacinsert_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <papertitle>Safely Learning Visuo-Tactile Feedback Policies in Real For Industrial Insertion</papertitle>
              <br>
              <!-- AUTHORS -->
              <b>Letian Fu</b>, Huang Huang, Lars Berscheid, Hui Li, Ken Goldberg, Sachin Chitta,
              <br>
              <!-- CONFERENCE -->
              <em>ICRA</em> 2023, <a href="https://arxiv.org/abs/2210.01340">arXiv</a>
              <p></p>
              <!-- SUMMARY -->
              <!-- We present a safe method to learn a visuo-tactile insertion policy that is robust against grasp pose variations while minimizing human inputs and collision between the robot and the environment. We achieve this by dividing the insertion task into a tactile-base grasp pose estimation phase and a visuoservo insertion phase. Using force-torque sensing, we also develop a safe self-supervised data collection pipeline that limits collision between the part and the surrounding environment. The resulting policy can insert the USB connector with 45 insertion successes on 45 different initial grasp poses. -->
            </td>
          </tr>

          <!-- EVO-NERF -->
          <tr onmouseout="evonerf_stop()" onmouseover="evonerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <div class="two" id='evonerf_under'>
                  <img src="images/evonerf_early_stop.png" width='180'/>
                </div>
                <video  width=115% height=100% muted autoplay loop id="evonerf_splash">
                  <source src="images/evonerf_splash.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function evonerf_start() {
                  document.getElementById('evonerf_splash').style.opacity = "0";
                  document.getElementById('evonerf_under').style.opacity = "1";
                }

                function evonerf_stop() {
                  document.getElementById('evonerf_splash').style.opacity = "1";
                  document.getElementById('evonerf_under').style.opacity = "0";
                }
                evonerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <a href="https://sites.google.com/view/evo-nerf">
                <papertitle>Evo-NeRF: Evolving NeRF for Sequential Robot Grasping</papertitle>
              </a>
              <br>
              <!-- AUTHORS -->
              Justin Kerr, <b>Letian Fu</b>, Huang Huang, Yahav Avigal, Matthew Tancik, Jeffrey Ichnowski, Angjoo Kanazawa, Ken Goldberg
              <br>
              <!-- CONFERENCE -->
              <em>CoRL</em> 2022, <b>Oral Presentation</b>, <a href="https://openreview.net/forum?id=Bxr45keYrf">OpenReview</a>
              <p></p>
              <!-- SUMMARY -->
              <!-- We propose Evo-NeRF with additional geometry regularizations improving performance in rapid capture settings to achieve real-time, updateable scene reconstruction for rapidly grasping table-top transparent objects. -->
              <!-- We train a NeRF-adapted grasping network learns to ignore floaters. -->
            </td>
          </tr>

          <!-- staxray -->
          <tr onmouseout="staxray_stop()" onmouseover="staxray_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <div class="two" id='staxray_destack'>
                  <img src="images/staxray_destack.png" width='180'/>
                </div>
                <img src="images/staxray_restack.png" width='180' id='staxray_restack'/>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function staxray_start() {
                  document.getElementById('staxray_restack').style.opacity = "0";
                  document.getElementById('staxray_destack').style.opacity = "1";
                }

                function staxray_stop() {
                  document.getElementById('staxray_restack').style.opacity = "1";
                  document.getElementById('staxray_destack').style.opacity = "0";
                }
                staxray_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->  
                <a href="https://sites.google.com/berkeley.edu/stax-ray">
                <papertitle>Mechanical Search on Shelves with Efficient
                  Stacking and Destacking of Objects</papertitle>
                </a>
              <br>
              <!-- AUTHORS -->
              <b>Letian Fu</b>*, Huang Huang*, Michael Danielczuk, Chung Min Kim,
                Zachary Tam, Jeffrey Ichnowski, Anelia Angelova, Brian Ichter, and
                Ken Goldberg, <small>*Equal contribution</small>
              <br>
              <!-- CONFERENCE -->
              <em>ISRR</em> 2022, <a href="https://arxiv.org/pdf/2207.02347.pdf">arXiv</a>
              <p></p>
              <!-- SUMMARY -->
              <!-- We develop two policies for lateral access mechanical search with stacked objects. Both policies utilize stacking and destacking actions and can reveal the -->
              <!-- target object with 82–100% success in simulation outperforming the baseline by up to 66%, and can achieve 67–100% success in physical experiments. -->
            </td>
          </tr>

          <!-- LEGS -->
          <tr onmouseout="legs_stop()" onmouseover="legs_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='legs_2'>
                  <img src="images/legs_splash_2.jpg" width='180'/>
                </div>
                <img src="images/legs_splash_1.jpg" width='180' id='legs_1'/>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function legs_start() {
                  document.getElementById('legs_1').style.opacity = "0";
                  document.getElementById('legs_2').style.opacity = "1";
                }

                function legs_stop() {
                  document.getElementById('legs_1').style.opacity = "1";
                  document.getElementById('legs_2').style.opacity = "0";
                }
                legs_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <a href="https://sites.google.com/view/legsexp-grasping">
                <papertitle>LEGS: Learning Efficient Grasp Sets for Exploratory Grasping</papertitle>
              </a>
              <br>
              <!-- AUTHORS -->
              <b>Letian Fu</b>, Michael Danielczuk, Ashwin Balakrishna, Daniel S. Brown, Jeffrey Ichnowski, Eugen Solowjow, Ken Goldberg,
              <br>
              <!-- CONFERENCE -->
              <em>ICRA</em> 2022, <a href="https://ieeexplore.ieee.org/abstract/document/9812138">IEEE</a>, <a href="https://arxiv.org/abs/2111.15002">arxiv</a>
              <p></p>
              <!-- SUMMARY -->
              <!-- We present Learned Efficient Grasp Sets, an algorithm which efficiently explores large sets of grasps by adaptively constructing a small active set of promising grasps. Experiments suggest that LEGS identiﬁes high-performing grasps more efficiently than baseline algorithms across 53 objects in simulation experiments and on three challenging objects in physical trials. We also propose a novel early stopping condition by computing a high-conﬁdence lower bound on the expected grasp performance. Simulation results suggest that this high-conﬁdence lower bound is highly accurate and tight. -->
            </td>
          </tr>

          <!-- SLAXRAY -->
          <tr onmouseout="slaxray_stop()" onmouseover="slaxray_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='slaxray_2'>
                  <img src="images/slaxray_splash_1.jpg" width='180'/>
                </div>
                <img src="images/slaxray_splash_2.jpg" width='180' id='slaxray_1'/>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function slaxray_start() {
                  document.getElementById('slaxray_1').style.opacity = "0";
                  document.getElementById('slaxray_2').style.opacity = "1";
                }

                function slaxray_stop() {
                  document.getElementById('slaxray_1').style.opacity = "1";
                  document.getElementById('slaxray_2').style.opacity = "0";
                }
                slaxray_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <papertitle>Mechanical Search on Shelves using a Novel “Bluction” Tool</papertitle>
              <br>
              <!-- AUTHORS -->
              Huang Huang, Michael Danielczuk, Chung Min Kim, <b>Letian Fu</b>, Zachary Tam, Jeffrey Ichnowski, Anelia Angelova, Brain Ichter, Ken Goldberg,
              <br>
              <!-- CONFERENCE -->
              <em>ICRA</em> 2022, <a href="https://ieeexplore.ieee.org/document/9811622">IEEE</a>, <a href="https://arxiv.org/abs/2201.08968">arXiv</a>
              <p></p>
              <!-- SUMMARY -->
              <!-- We present SLAX-RAY, a lateral access mechanical search pipeline. SLAX-RAY uses the bluction tool, a novel perception algorithm and a new search policy BluctionDAR that uses both pushing and suction actions to reveal a target object in a shelf environment. Experiments in simulated and physical settings demonstrate that the increased action set improves success rate by 26% and 67%, respectively, over pushing-only baselines.  -->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p>
                Source code taken from <a href="https://jonbarron.info/">Jon Barron's site.</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
