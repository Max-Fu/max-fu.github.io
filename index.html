<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <title>Letian (Max) Fu</title>
  <meta name="author" content="Letian (Max) Fu">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <!-- Add Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>

<body>
  <div class="container">
    <table>
      <tr>
        <td style="width:60%;vertical-align:middle">
          <name>Letian (Max) Fu</name>
          <p>
            I am a second year PhD student at UC Berkeley advised by Professor <a href="https://goldberg.berkeley.edu/">Ken Goldberg</a>. 
            I am also a research intern at NVIDIA, working with <a href="https://jimfan.me/">Jim Fan</a> and <a href="https://yukezhu.me/">Yuke Zhu</a>. 
            I am interested in building self-supervised multi-modal models for robot learning. 
          </p>
          <p>
            I received B.A./M.S. in Computer Science and Applied Mathematics from UC Berkeley in 2023. 
          </p>
          <p style="text-align:center">
            <a href="mailto:max.fu.letian@berkeley.edu"><i class="fas fa-envelope"></i> Email</a> &nbsp/&nbsp
            <a href="https://scholar.google.com/citations?user=aWot7UgAAAAJ&hl"><i class="fas fa-graduation-cap"></i> Google Scholar</a> &nbsp/&nbsp
            <a href="https://www.linkedin.com/in/letian-max-fu-8b672b134/"><i class="fab fa-linkedin"></i> LinkedIn</a> &nbsp/&nbsp
            <a href="CV.pdf"><i class="fas fa-file-alt"></i> Resume</a>
          </p>
        </td>
        <td style="width:40%;vertical-align:middle">
          <div class="one">
            <img alt="profile photo" src="images/profile.jpg">
          </div>
        </td>
      </tr>
    </table>

    <heading>Selected Publications</heading>

    <table>
      <!-- OTTER -->
      <tr>
        <td style="width:40%;vertical-align:middle">
          <div class="one">
            <img src="images/otter_figure.png" alt="OTTER figure">
          </div>
        </td>
        <td style="width:60%;vertical-align:middle">
          <papertitle><a href="https://ottervla.github.io/">ðŸ¦¦OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction</a></papertitle>
          <br>
          Huang Huang*, Fangchen Liu*, <b>Letian Fu*</b>, Tingfan Wu, Mustafa Mukadam, Jitendra Malik, Ken Goldberg, Pieter Abbeel
          <br>
          <small>*Equal contribution</small>
          <br>
          <em>Arxiv</em> 2025, <a href="http://arxiv.org/abs/2503.03734"><i class="fas fa-file-pdf"></i> Paper</a>, <a href="https://ottervla.github.io/"><i class="fas fa-globe"></i> Website</a>, <a href="https://github.com/Max-Fu/otter"><i class="fab fa-github"></i> Code</a>
        </td>
      </tr>

      <!-- ICRT -->
      <tr>
        <td style="width:40%;vertical-align:middle">
          <div class="one">
            <img src="images/icrt_splash.png" alt="ICRT figure">
          </div>
        </td>
        <td style="width:60%;vertical-align:middle">
          <papertitle><a href="https://icrt.dev/">In-Context Imitation Learning via Next-Token Prediction</a></papertitle>
          <br>
          <b>Letian Fu*</b>, Huang Huang*, Gaurav Datta*, Lawrence Yunliang Chen, William Chung-Ho Panitch, Fangchen Liu, Hui Li, Ken Goldberg 
          <br>
          <small>*Equal contribution</small>
          <br>
          <em>ICRA</em> 2025, <a href="https://icrt.dev/files/icrt.pdf"><i class="fas fa-file-pdf"></i> Paper</a>, <a href="https://icrt.dev/"><i class="fas fa-globe"></i> Website</a>, <a href="https://github.com/Max-Fu/icrt"><i class="fab fa-github"></i> Code</a>, <a href="https://huggingface.co/datasets/Ravenh97/ICRT-MT"><i class="fas fa-database"></i> Dataset</a>
        </td>
      </tr>

      <!-- TVL -->
      <tr>
        <td style="width:40%;vertical-align:middle">
          <div class="one">
            <img src="images/tvl_splash.jpg" alt="TVL figure">
          </div>
        </td>
        <td style="width:60%;vertical-align:middle">
          <papertitle><a href="https://tactile-vlm.github.io">A Touch, Vision, and Language Dataset for Multimodal Alignment</a></papertitle>
          <br>
          <b>Letian Fu</b>, Gaurav Datta*, Huang Huang*, William Chung-Ho Panitch*, Jaimyn Drake*, Joseph Ortiz, Mustafa Mukadam, Mike Lambeta, Roberto Calandra, Ken Goldberg 
          <br>
          <small>*Equal contribution</small>
          <br>
          <em>ICML</em> 2024, <b>Oral Presentation</b>, <a href="https://openreview.net/forum?id=tFEOOH9eH0"><i class="fas fa-file-alt"></i> OpenReview</a>, <a href="https://tactile-vlm.github.io/"><i class="fas fa-globe"></i> Website</a>, <a href="https://github.com/Max-Fu/tvl"><i class="fab fa-github"></i> Code</a>, <a href="https://huggingface.co/datasets/mlfu7/Touch-Vision-Language-Dataset"><i class="fas fa-database"></i> Dataset</a>
        </td>
      </tr>

      <!-- CrossMAE -->
      <tr>
        <td style="width:40%;vertical-align:middle">
          <div class="one">
            <img src="images/crossmae_website.jpeg" alt="CrossMAE figure">
          </div>
        </td>
        <td style="width:60%;vertical-align:middle">
          <papertitle><a href="https://crossmae.github.io/">Rethinking Patch Dependence for Masked Autoencoders</a></papertitle>
          <br>
          <b>Letian Fu</b>*, Long Lian*, Renhao Wang, Baifeng Shi, Xudong Wang, Adam Yalaâ€ , Trevor Darrellâ€ , Alexei A Efrosâ€ , Ken Goldbergâ€ 
          <br>
          <small>*Equal contribution, â€ Equal advising</small>
          <br>
          <em>TMLR</em> 2025, <a href="https://arxiv.org/abs/2401.14391"><i class="fas fa-file-pdf"></i> Paper</a>, <a href="https://crossmae.github.io/"><i class="fas fa-globe"></i> Website</a>, <a href="https://github.com/TonyLianLong/CrossMAE"><i class="fab fa-github"></i> Code</a>
        </td>
      </tr>

      <!-- RPT -->
      <tr>
        <td style="width:40%;vertical-align:middle">
          <div class="one">
            <video width='300' muted autoplay loop id="rpt_splash_1">
              <source src="images/rpt_splash_1.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </td>
        <td style="width:60%;vertical-align:middle">
          <papertitle><a href="https://robotic-pretrained-transformer.github.io/">Robot Learning with Sensorimotor Pre-training</a></papertitle>
          <br>
          Ilija Radosavovic, Baifeng Shi, <b>Letian Fu</b>, Ken Goldberg, Trevor Darrell, Jitendra Malik
          <br>
          <em>CoRL</em> 2023, <b>Oral Presentation</b>, <a href="https://openreview.net/forum?id=3gh9hf3R6x"><i class="fas fa-file-alt"></i> OpenReview</a>
        </td>
      </tr>

      <!-- TacInsertion -->
      <tr>
        <td style="width:40%;vertical-align:middle">
          <div class="one">
            <img src="images/tactile_insertion.jpg" alt="Tactile Insertion figure" id='tactile_insertion_1'>
          </div>
        </td>
        <td style="width:60%;vertical-align:middle">
          <papertitle>Safely Learning Visuo-Tactile Feedback Policies in Real For Industrial Insertion</papertitle>
          <br>
          <b>Letian Fu</b>, Huang Huang, Lars Berscheid, Hui Li, Ken Goldberg, Sachin Chitta,
          <br>
          <em>ICRA</em> 2023, <a href="https://arxiv.org/abs/2210.01340"><i class="fas fa-file-pdf"></i> arXiv</a>
        </td>
      </tr>

      <!-- EVO-NERF -->
      <tr>
        <td style="width:40%;vertical-align:middle">
          <div class="one">
            <video width='300' muted autoplay loop id="evonerf_splash">
              <source src="images/evonerf_splash.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </td>
        <td style="width:60%;vertical-align:middle">
          <a href="https://sites.google.com/view/evo-nerf">
            <papertitle>Evo-NeRF: Evolving NeRF for Sequential Robot Grasping</papertitle>
          </a>
          <br>
          Justin Kerr, <b>Letian Fu</b>, Huang Huang, Yahav Avigal, Matthew Tancik, Jeffrey Ichnowski, Angjoo Kanazawa, Ken Goldberg
          <br>
          <em>CoRL</em> 2022, <b>Oral Presentation</b>, <a href="https://openreview.net/forum?id=Bxr45keYrf"><i class="fas fa-file-alt"></i> OpenReview</a>
        </td>
      </tr>

      <!-- staxray -->
      <tr>
        <td style="width:40%;vertical-align:middle">
          <div class="one">
            <img src="images/staxray_restack.png" alt="staxray figure">
          </div>
        </td>
        <td style="width:60%;vertical-align:middle">
          <a href="https://sites.google.com/berkeley.edu/stax-ray">
            <papertitle>Mechanical Search on Shelves with Efficient
              Stacking and Destacking of Objects</papertitle>
          </a>
          <br>
          <b>Letian Fu</b>*, Huang Huang*, Michael Danielczuk, Chung Min Kim,
            Zachary Tam, Jeffrey Ichnowski, Anelia Angelova, Brian Ichter, and
            Ken Goldberg, <small>*Equal contribution</small>
          <br>
          <em>ISRR</em> 2022, <a href="https://arxiv.org/pdf/2207.02347.pdf"><i class="fas fa-file-pdf"></i> arXiv</a>
        </td>
      </tr>

      <!-- LEGS -->
      <tr>
        <td style="width:40%;vertical-align:middle">
          <div class="one">
            <img src="images/legs_splash_1.jpg" alt="LEGS figure">
          </div>
        </td>
        <td style="width:60%;vertical-align:middle">
          <a href="https://sites.google.com/view/legsexp-grasping">
            <papertitle>LEGS: Learning Efficient Grasp Sets for Exploratory Grasping</papertitle>
          </a>
          <br>
          <b>Letian Fu</b>, Michael Danielczuk, Ashwin Balakrishna, Daniel S. Brown, Jeffrey Ichnowski, Eugen Solowjow, Ken Goldberg,
          <br>
          <em>ICRA</em> 2022, <a href="https://ieeexplore.ieee.org/abstract/document/9812138"><i class="fas fa-file-alt"></i> IEEE</a>, <a href="https://arxiv.org/abs/2111.15002"><i class="fas fa-file-pdf"></i> arxiv</a>
        </td>
      </tr>

      <!-- SLAXRAY -->
      <tr>
        <td style="width:40%;vertical-align:middle">
          <div class="one">
            <img src="images/slaxray_splash_1.jpg" alt="SLAXRAY figure">
          </div>
        </td>
        <td style="width:60%;vertical-align:middle">
          <papertitle>Mechanical Search on Shelves using a Novel "Bluction" Tool</papertitle>
          <br>
          Huang Huang, Michael Danielczuk, Chung Min Kim, <b>Letian Fu</b>, Zachary Tam, Jeffrey Ichnowski, Anelia Angelova, Brain Ichter, Ken Goldberg,
          <br>
          <em>ICRA</em> 2022, <a href="https://ieeexplore.ieee.org/document/9811622"><i class="fas fa-file-alt"></i> IEEE</a>, <a href="https://arxiv.org/abs/2201.08968"><i class="fas fa-file-pdf"></i> arXiv</a>
        </td>
      </tr>
    </table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:0px">
          <p>
            <i class="fas fa-code"></i> Source code taken from <a href="https://jonbarron.info/">Jon Barron's site</a>.
          </p>
        </td>
      </tr>
    </tbody></table>
  </div>
</body>

</html>
